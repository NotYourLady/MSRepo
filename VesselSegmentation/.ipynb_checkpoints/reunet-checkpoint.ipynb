{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667ce913-015d-45eb-8255-bc07031569b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "199b30c4-e7c8-4778-9c03-bec4d33eb611",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class norm_act(nn.Module):\n",
    "    def __init__(self, channels, act=nn.ReLU()):\n",
    "        super(norm_act, self).__init__()\n",
    "        self.act = act\n",
    "        self.norm = nn.InstanceNorm3d(channels, affine=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02fbce35-c8e6-4e90-a82c-0fef59b075b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
    "                 stride=1, padding=1):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.act = norm_act(in_channels)\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                              kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                              padding_mode=\"reflect\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e638e3a8-647c-42aa-902f-256c917d7b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class stem(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
    "                 stride=1, padding=1):\n",
    "        super(stem, self).__init__()\n",
    "        \n",
    "        self.main_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                  kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                  padding_mode=\"reflect\"),\n",
    "            conv_block(in_channels=out_channels, out_channels=out_channels,\n",
    "                                  kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        )\n",
    "        self.shortcut_conv =  nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                  kernel_size=(1,1,1), stride=stride, padding=0,\n",
    "                                  padding_mode=\"replicate\"),\n",
    "            norm_act(out_channels, nn.Identity())\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        main = self.main_conv(x)\n",
    "        shortcut = self.shortcut_conv(x)\n",
    "        return main + shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eaa36be-3e86-49d5-8ae8-f67c93fdd2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class residual_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=1,\n",
    "                 drop=0):\n",
    "        super(residual_block, self).__init__()\n",
    "        \n",
    "        self.main_conv = nn.Sequential(\n",
    "            conv_block(in_channels=in_channels, out_channels=out_channels,\n",
    "                                  kernel_size=kernel_size, stride=stride, padding=kernel_size//2),\n",
    "            conv_block(in_channels=out_channels, out_channels=out_channels,\n",
    "                                  kernel_size=kernel_size, stride=1, padding=1)\n",
    "        )\n",
    "        self.shortcut_conv =  nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                  kernel_size=(1,1,1), stride=stride, padding=0,\n",
    "                                  padding_mode=\"replicate\"),\n",
    "            norm_act(out_channels, nn.Identity())\n",
    "        )\n",
    "        self.dropout = nn.Dropout3d(p=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        main = self.main_conv(x)\n",
    "        shortcut = self.shortcut_conv(x)\n",
    "        out = main + shortcut\n",
    "        return self.dropout(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bdd703d-3b2b-4850-acba-834cdba72e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class upsample_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,\n",
    "                 kernel_size=3,\n",
    "                 stride=2, padding=2,\n",
    "                 drop=0):\n",
    "        super(upsample_block, self).__init__()\n",
    "        \n",
    "        self.unconv = torch.nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride,\n",
    "                                               padding=1, output_padding=stride-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.unconv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6438a5-c670-4423-95ad-7b10be49eea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class attention_gate(nn.Module):\n",
    "    def __init__(self, in_channels1, in_channels2, intermediate_channels, act=nn.LeakyReLU()):\n",
    "        super(attention_gate, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(in_channels=in_channels1, out_channels=intermediate_channels,\n",
    "                              kernel_size=1, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv3d(in_channels=in_channels2, out_channels=intermediate_channels,\n",
    "                              kernel_size=1, stride=1, padding=0)\n",
    "        self.conv = nn.Conv3d(in_channels=intermediate_channels, out_channels=1,\n",
    "                              kernel_size=1, stride=1, padding=0)\n",
    "        self.act = act\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1_conv = self.conv1(x1)\n",
    "        x2_conv = self.conv2(x2)\n",
    "        inter = self.act(x1_conv + x2_conv)\n",
    "        inter = self.sigmoid(self.conv(inter))\n",
    "        return x1*inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719c4887-26c2-4ed3-adfe-2a49c9e72e81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class attention_concat(nn.Module):\n",
    "    def __init__(self, main_channels, skip_channels, act=nn.LeakyReLU()):\n",
    "        super(attention_concat, self).__init__()\n",
    "        self.att_gate = attention_gate(skip_channels, main_channels, main_channels)\n",
    "        \n",
    "    def forward(self, main, skip):\n",
    "        attention_across = self.att_gate(skip, main)\n",
    "        return torch.cat([main, attention_across], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ac68eaa-3e02-42f7-b81a-ad5b15e91ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, \n",
    "                 drop=0.2,\n",
    "                 dropout_change_per_layer=0.0,\n",
    "                 channels_coef=16,\n",
    "                 out_act=nn.Sigmoid(),\n",
    "                 use_input_noise=False):\n",
    "        super(ResUNet, self).__init__()\n",
    "        lc = [channels_coef, 2*channels_coef, 4*channels_coef, 8*channels_coef, 16*channels_coef]\n",
    "        \n",
    "        self.stem = stem(in_channels=in_channels, out_channels=lc[0])\n",
    "        \n",
    "        self.encoder = nn.ModuleList(\n",
    "           [residual_block(lc[0], lc[1], stride=2, drop=drop),\n",
    "            residual_block(lc[1], lc[2], stride=2, drop=drop + 1*dropout_change_per_layer),\n",
    "            residual_block(lc[2], lc[3], stride=2, drop=drop + 2*dropout_change_per_layer),\n",
    "            residual_block(lc[3], lc[4], stride=2, drop=drop + 3*dropout_change_per_layer)]\n",
    "        )\n",
    "        self.bridge = nn.Sequential(\n",
    "            conv_block(lc[4], lc[4]),\n",
    "            conv_block(lc[4], lc[4])\n",
    "        )    \n",
    "        self.decoder = nn.ModuleList([\n",
    "           nn.ModuleList([upsample_block(lc[4], lc[3]),\n",
    "                          attention_concat(lc[3], lc[3]),\n",
    "                          residual_block(2*lc[3], lc[3], stride=1)]),\n",
    "            \n",
    "           nn.ModuleList([upsample_block(lc[3], lc[2]),\n",
    "                          attention_concat(lc[2], lc[2]),\n",
    "                          residual_block(2*lc[2], lc[2], stride=1)]),\n",
    "            \n",
    "           nn.ModuleList([upsample_block(lc[2], lc[1]),\n",
    "                          attention_concat(lc[1], lc[1]),\n",
    "                          residual_block(2*lc[1], lc[1], stride=1)]),\n",
    "            \n",
    "           nn.ModuleList([upsample_block(lc[1], lc[0]),\n",
    "                          attention_concat(lc[0], lc[0]),\n",
    "                          residual_block(2*lc[0], lc[0], stride=1)]),\n",
    "        ])\n",
    "        \n",
    "        self.output_block = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=lc[0], out_channels=1, kernel_size=1, stride=1, padding=0),\n",
    "            out_act\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        skip_layers = []\n",
    "        x = self.stem(x)\n",
    "        skip_layers.append(x)\n",
    "        \n",
    "        #encode\n",
    "        for enc_blok in self.encoder:\n",
    "            x = enc_blok(x)\n",
    "            skip_layers.append(x)\n",
    "        \n",
    "        #bridge\n",
    "        x = self.bridge(x)\n",
    "        \n",
    "        #decode\n",
    "        for idx, dec_blok in enumerate(self.decoder):\n",
    "            x = dec_blok[0](x)\n",
    "            x = dec_blok[1](x, skip_layers[3-idx])\n",
    "            x = dec_blok[2](x)\n",
    "            \n",
    "        out = self.output_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a0fbd82-cd2d-4326-89b1-89c34d60066d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1, 1, 64, 64, 64)\n",
    "RN = ResUNet(in_channels=1)\n",
    "print(RN(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb425c45-55db-41ec-adb3-46b00b853b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af4fa9d-54ca-4715-bf30-9d4ab3edf55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635052e6-c316-4929-9b3e-d6729e7aae6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
