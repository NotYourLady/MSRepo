{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bf6822-f89e-4f50-8b45-59c97f052057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59577c-e0b5-4c80-9e8f-6aa67906e257",
   "metadata": {},
   "source": [
    "**<h4> Embeddings** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be126e1-169a-4c85-89ff-220eddb79186",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Construct the embeddings from patch, position embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, \n",
    "                 img_shape, patch_shape,\n",
    "                 hidden_size=None, drop=0.0):\n",
    "        super(Embeddings, self).__init__()\n",
    "\n",
    "        self.patch_grid_shape = torch.tensor(img_shape)//torch.tensor(patch_shape)\n",
    "        if not torch.prod(torch.tensor(img_shape)==self.patch_grid_shape * torch.tensor(patch_shape)):\n",
    "            raise RuntimeError(\n",
    "                f\"Embeddings::__init__::ERROR: Bad <img_shape>({img_shape}) or <patch_shape>({patch_shape})\"\n",
    "            )\n",
    "        assert torch.prod(torch.tensor(img_shape)==self.patch_grid_shape * torch.tensor(patch_shape))\n",
    "        self.n_patches = torch.prod(self.patch_grid_shape)\n",
    "\n",
    "        \n",
    "        if len(img_shape) == 3:\n",
    "            self.is3d=True\n",
    "        else:\n",
    "            self.is3d=False\n",
    "            \n",
    "        self.hidden_size = hidden_size\n",
    "        if self.hidden_size is None:\n",
    "            self.hidden_size = torch.prod(torch.tensor(patch_shape))\n",
    "        \n",
    "        if self.is3d:\n",
    "            self.patch_embeddings = nn.Conv3d(in_channels=in_channels,\n",
    "                                              out_channels=self.hidden_size,\n",
    "                                              kernel_size=patch_shape,\n",
    "                                              stride=patch_shape)\n",
    "        else:\n",
    "            self.patch_embeddings = nn.Conv2d(in_channels=in_channels,\n",
    "                                              out_channels=self.hidden_size,\n",
    "                                              kernel_size=patch_shape,\n",
    "                                              stride=patch_shape)\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, self.n_patches, self.hidden_size))\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)  # (B, hidden, patches_n)\n",
    "        x = x.flatten(2) # (B, hidden, patches_n)\n",
    "        x = x.transpose(-1, -2)  # (B, n_patches, hidden)\n",
    "\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7859aac7-3e43-4e86-8fba-240ddf7b954c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 4096]) \n",
      "patch_grid_shape: tensor([4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randn(1, 1, 64, 64, 64)\n",
    "m1 = Embeddings(img_shape=(64, 64, 64), patch_shape=(16, 16, 16),\n",
    "               in_channels=1, hidden_size=4096)\n",
    "\n",
    "out = m1(t1)\n",
    "print(out.shape, \"\\npatch_grid_shape:\", m1.patch_grid_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223bf521-67f3-4a68-9f05-b1f41ef4fd9c",
   "metadata": {},
   "source": [
    "**<h4> Projector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dee4fe3-ab86-4ca4-a99a-2a0ea7a344e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_features, out_features, act=nn.ReLU()):\n",
    "        super(Projector, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, out_features, bias=True)\n",
    "        #self.fc2 = nn.Linear(out_features, out_features, bias=True)\n",
    "        self.act = act\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        #x = self.act(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44dc56d-9c6d-46be-8231-ad181a73187f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 3, 64*64)\n",
    "m = Projector(64*64, 64*8)\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6f146-49a2-4214-b589-18d0d855e574",
   "metadata": {},
   "source": [
    "**<h4> Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c040516-fa27-4181-b660-c1fd255a6fbf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.query = Projector(in_features, out_features)\n",
    "        self.key = Projector(in_features, out_features)\n",
    "        self.value = Projector(in_features, out_features)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        logits = torch.einsum('bij,bkj->bik', q, k)\n",
    "        scores = self.softmax(logits)\n",
    "        out = torch.einsum('bij,bjk->bik', scores, v)\n",
    "        return(out)\n",
    "\n",
    "\n",
    "class MultyHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, in_features, n_heads):\n",
    "        super(MultyHeadSelfAttention, self).__init__()\n",
    "\n",
    "        head_features = in_features//n_heads\n",
    "        self.att_list = nn.ModuleList([SelfAttention(in_features, head_features) for _ in range(n_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_list = []\n",
    "        for att in self.att_list:\n",
    "            out_list.append(att(x))\n",
    "        out = torch.cat(out_list, -1)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b71619-b68f-4be8-9bfe-aeacdae20998",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4096])\n",
      "torch.Size([5, 3, 512])\n",
      "torch.Size([5, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3, 64*64)\n",
    "print(x.shape)\n",
    "\n",
    "m = SelfAttention(64*64, 64*8)\n",
    "print(m(x).shape)\n",
    "\n",
    "m = MultyHeadSelfAttention(64*64, 8)\n",
    "print(m(x).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0525e4d-d276-4ed6-a3d5-95ea5ea99b58",
   "metadata": {},
   "source": [
    "**<h4> Mlp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200263d5-c223-42e3-a5b5-db4c4dd422d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, mlp_dim,\n",
    "                 act=torch.nn.functional.gelu, drop=0.0):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, mlp_dim)\n",
    "        self.fc2 = nn.Linear(mlp_dim, in_features)\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e882e3a7-1363-46c8-abb6-73fd60eec1aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4096])\n",
      "torch.Size([5, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3, 64*64)\n",
    "print(x.shape)\n",
    "\n",
    "m = Mlp(64*64, 64*8)\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdae32e-87bf-45d1-adb3-3b7b82e95d11",
   "metadata": {},
   "source": [
    "**<h4> TransformerBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dd2db69-cd79-4067-b941-16af8ca1ddc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, in_features, attention_heads, mlp_dim, drop=0.0):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(in_features, eps=1e-6)\n",
    "        self.norm2 = nn.LayerNorm(in_features, eps=1e-6)\n",
    "        self.mlp = Mlp(in_features, mlp_dim, drop=drop)\n",
    "        self.attn = MultyHeadSelfAttention(in_features, attention_heads)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(x)\n",
    "        x = self.norm1(x)\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.norm1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36d3efb1-757f-43ee-9237-d55cb1364b4d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4096])\n",
      "torch.Size([5, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3, 64*64)\n",
    "print(x.shape)\n",
    "\n",
    "m = TransformerBlock(64*64, 8, 64*8)\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac1f9e-6a5f-447e-bda8-c242044a499f",
   "metadata": {},
   "source": [
    "**<h4> Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "465eaacc-6d97-42d6-8693-feba0fbd3b29",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features,\n",
    "                 attention_heads,\n",
    "                 mlp_dim,\n",
    "                 transformer_layers,\n",
    "                 drop=0.0):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.block_list = nn.ModuleList()\n",
    "        self.norm = nn.LayerNorm(in_features, eps=1e-6)\n",
    "        for _ in range(transformer_layers):\n",
    "            block = TransformerBlock(in_features=in_features, attention_heads=attention_heads,\n",
    "                                     mlp_dim=mlp_dim, drop=drop)\n",
    "            self.block_list.append(copy.deepcopy(block))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.block_list:\n",
    "            x = block(x)\n",
    "        out = self.norm(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 img_shape,\n",
    "                 patch_shape,\n",
    "                 hidden_size,\n",
    "                 attention_heads,\n",
    "                 mlp_dim, \n",
    "                 transformer_layers,\n",
    "                 drop=0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(in_channels=in_channels, img_shape=img_shape,\n",
    "                                     patch_shape=patch_shape, hidden_size=hidden_size, drop=drop)\n",
    "        self.encoder = TransformerEncoder(in_features=self.embeddings.hidden_size.item(), attention_heads=attention_heads,\n",
    "                                          mlp_dim=mlp_dim, transformer_layers=transformer_layers,\n",
    "                                          drop=drop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded = self.encoder(embedding_output)  \n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045ab388-6876-41f0-a10d-fa834f51e746",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 64, 64, 64])\n",
      "torch.Size([1, 64, 4096])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 32, 64, 64, 64)\n",
    "print(x.shape)\n",
    "\n",
    "m = Transformer(in_channels=32, img_shape=(64,64,64), patch_shape=(16,16,16),\n",
    "                hidden_size=None, attention_heads=8, mlp_dim=128, \n",
    "                transformer_layers=6)\n",
    "\n",
    "out = m(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7df869-9bbc-4319-af59-2b9b73c8fc6e",
   "metadata": {},
   "source": [
    "**<h4> Conv2dReLU, DecoderBlock**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349b1358-c4c2-440d-85ab-611352495a97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Conv3dReLU(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 kernel_size,\n",
    "                 padding=0,\n",
    "                 stride=1,\n",
    "                 use_batchnorm=True):\n",
    "        conv = nn.Conv3d(in_channels, out_channels,\n",
    "                         kernel_size, stride=stride,\n",
    "                         padding=padding, bias=not (use_batchnorm))\n",
    "        relu = nn.ReLU(inplace=True)\n",
    "        bn = nn.BatchNorm3d(out_channels)\n",
    "        super(Conv3dReLU, self).__init__(conv, bn, relu)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 skip_channels=0,\n",
    "                 use_batchnorm=True,\n",
    "                 resample=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv3dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.conv2 = Conv3dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "        )\n",
    "        self.resample = nn.Upsample(scale_factor=resample, mode='trilinear', align_corners=None)\n",
    "\n",
    "\n",
    "    def forward(self, x, skip=None):\n",
    "        x = self.resample(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a723b0f4-ded7-465d-ae21-2ab12579a01e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 128, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 64, 64, 64)\n",
    "m = DecoderBlock(in_channels=3,\n",
    "                 out_channels=5,\n",
    "                 skip_channels=0,\n",
    "                 use_batchnorm=True,\n",
    "                 resample=2)\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb10b06-d165-4f6e-b2ff-0fbaa18a3d11",
   "metadata": {},
   "source": [
    "**<h4> DecoderCup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be0201f5-7b41-467b-b85a-1cdfe6bed4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderCup(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size,\n",
    "                 out_channels,\n",
    "                 patch_grid_shape,\n",
    "                 skip_channels=0,\n",
    "                 resample=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_grid_shape = patch_grid_shape\n",
    "        \n",
    "        blocks = [\n",
    "            DecoderBlock(in_channels=hidden_size, out_channels=hidden_size//2, resample=2),\n",
    "            DecoderBlock(in_channels=hidden_size//2, out_channels=hidden_size//4, resample=2),\n",
    "            DecoderBlock(in_channels=hidden_size//4, out_channels=hidden_size//8, resample=2),\n",
    "            DecoderBlock(in_channels=hidden_size//8, out_channels=out_channels, resample=2),\n",
    "        ]\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, hidden_states, features=None):\n",
    "        B, n_patch, hidden = hidden_states.size()  # reshape from (B, n_patch, hidden) to (B, h, w, hidden)\n",
    "        H, W, D = self.patch_grid_shape\n",
    "        x = hidden_states.permute(0, 2, 1)\n",
    "        x = x.contiguous().view(B, hidden, H, W, D)\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            x = decoder_block(x, skip=None)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd4832c6-e119-44f9-8414-e5a6565815dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 4, 4, 4])\n",
      "torch.Size([1, 1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 64, 512)\n",
    "m = DecoderCup(hidden_size=512,\n",
    "               out_channels=1,\n",
    "               patch_grid_shape=(4, 4, 4))\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde8373-a979-4575-b094-677af94c7f94",
   "metadata": {},
   "source": [
    "**<h4> VisionTransformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "435c4ef0-8bc4-4521-9f6f-fa89b66ad4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 img_shape=(64, 64, 64),\n",
    "                 patch_shape=(16, 16, 16),\n",
    "                 in_channels=1,\n",
    "                 hidden_size=None,\n",
    "                 transformer_layers=8,\n",
    "                 attention_heads=8,\n",
    "                 mlp_dim=1024, \n",
    "                 drop=0.0):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.transformer = Transformer(in_channels=in_channels,\n",
    "                                       img_shape=img_shape,\n",
    "                                       patch_shape=patch_shape,\n",
    "                                       hidden_size=hidden_size,\n",
    "                                       attention_heads=attention_heads,\n",
    "                                       mlp_dim=mlp_dim, \n",
    "                                       transformer_layers=transformer_layers)\n",
    "        self.decoder = DecoderCup(hidden_size=4096,\n",
    "                                  out_channels=1,\n",
    "                                  patch_grid_shape=self.transformer.embeddings.patch_grid_shape)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.transformer(x)  # (B, n_patch, hidden)\n",
    "        x = self.decoder(x)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f1c0a3c-75db-4773-9b2d-5773d95cb2ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64, 64])\n",
      "torch.Size([1, 1, 64, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 64, 64, 64)\n",
    "m = VisionTransformer(img_shape=(64, 64, 64),\n",
    "                      patch_shape=(16, 16, 16),\n",
    "                      in_channels=1,\n",
    "                      hidden_size=None,\n",
    "                      transformer_layers=8,\n",
    "                      attention_heads=8,\n",
    "                      mlp_dim=1024, \n",
    "                      drop=0.0)\n",
    "print(m(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dee451-1312-4920-8617-05b7aaa2c2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11acc8cf-f36e-4a7a-90b2-549e2801fe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a4bcef4-a965-4c7b-b2a1-5a2807771541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname('../.'))\n",
    "from ml.models.CBTN import VisionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7144b89e-9804-4cff-801f-4037d54abdb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 64, 64, 64])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 1, 64, 64, 64)\n",
    "m = VisionTransformer(img_shape=(64, 64, 64),\n",
    "                      patch_shape=(16, 16, 16),\n",
    "                      in_channels=1,\n",
    "                      hidden_size=None,\n",
    "                      transformer_layers=8,\n",
    "                      attention_heads=8,\n",
    "                      mlp_dim=1024, \n",
    "                      drop=0.0)\n",
    "m(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca5abc-8ef8-4903-ad0c-f05d0dc331c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
